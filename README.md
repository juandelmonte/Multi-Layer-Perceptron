# Multi Layer Perceptron (MLP)

As an engineer with a strong interest in data science and analytics, I added a Multi-Layer Perceptron (MLP) model to my portfolio using state-of-the-art numerical computing with the numpy library. MLPs are a fascinating type of Artificial Neural Network that have numerous applications in machine learning and artificial intelligence systems.

What I find particularly interesting about MLPs is their ability not only to make accurate predictions but also to identify patterns in the data, which comes first can help enhance our understanding of it and improve our analytical capabilities.

My primary goal for creating this MLP code was to gain a deeper understanding of the fundamental principles behind Artificial Neural Networks, such as backpropagation, stochastic gradient descent (SGD), and statistical approaches for evaluating models on unknown data. However, I also wanted to showcase the things that can be achieved with MLPs in machine learning, delivering value to society and enhancing business processes when applied propperly.

I've used MLPs to solve several challenging machine learning problems, including handwriting recognition and predicting the resistance (drag) of different yacht designs, which is another approach to classical engineering computations. I also tackled the classic Kaggle problem of bank fraud by building an autoencoder and classifying transactions based on the reconstruction of errors.

It's thrilling to see the MLP improve with experience by producing more accurate classification models and regression outputs that are closer to real values. But to ensure that the model generalizes well on unknown data, evaluation and statistical approaches are crucial.

One of the most fascinating aspects of MLPs is the feedforward method, where the input is transformed into an output using a set of functions that are applied sequentially or nested functions. This approach produces a set of transformations that are made to the input.

To train the MLP, I utilized stochastic approximation of gradient descent optimization, moving in the opposite direction of the gradient of a cost function to make the MLP produce outputs that are closer to the desired outputs. Instead of using the entire dataset to train the MLP, random samples can be used, making it stochastic and improving performance and generalization.

Finally, the backward propagation of errors uses the gradient with derivatives and the chain rule to update the weights and biases in the MLP. This technique provides a fascinating and exciting journey into the world of machine learning, where we explore the intersection of mathematics, statistics, and computer science.

Overall, working with MLPs is a thrilling experience, and I'm excited to add this code to my portfolio. If you're looking for an exciting challenge, let's dive in and have some fun exploring the world of MLPs!